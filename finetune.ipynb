{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from pokemon_dataset import PokemonImageDataset\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
    "\n",
    "test_dataset = torch.load('./dataset.pth')\n",
    "total_samples = 640\n",
    "batch_size = 32  # Adjust the batch size as needed\n",
    "imgs = []\n",
    "names = []\n",
    "name_idxs = []\n",
    "for example in test_dataset:\n",
    "    imgs.append(example[0])\n",
    "    names.append(example[1][0])\n",
    "    name_idxs.append(example[1][1])\n",
    "\n",
    "loss_img = torch.nn.CrossEntropyLoss()\n",
    "loss_txt = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "losses = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    # Loop through the test set in batches\n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        cur_end_idx = min(i+batch_size, len(test_dataset))\n",
    "\n",
    "        images = torch.stack([preprocess(image) for image in imgs[i:cur_end_idx]]).to(device)\n",
    "        text_inputs = torch.cat([\n",
    "            clip.tokenize(f\"a photo of Pokemon named {c}\")\n",
    "            for c in names[i:cur_end_idx]\n",
    "        ]).to(device)\n",
    "\n",
    "        logits_per_image, logits_per_text = model(images, text_inputs)\n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += total_loss.item()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    average_loss = epoch_loss / (total_samples / batch_size)\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Average Loss: {average_loss}\")\n",
    "    torch.save(model.state_dict(), f'checkpoint_{epoch}.pt')\n",
    "    losses.append(average_loss)\n",
    "\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), losses, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
